"Sentence Tokenize"

# Standard Library
import re
from typing import Any, Dict, List

# 3rd party libraries
import nltk

class SentenceTokenize:
    extra_tokens = ["ã€‚"]
    other_tokens = [
        "Ö‰",
        "ØŸ",
        "Û”",
        "Ü€",
        "Ü",
        "Ü‚",
        "ß¹",
        "à¥¤",
        "à¥¥",
        "áŠ",
        "á‹",
        "á¢",
        "á§",
        "á¨",
        "á™®",
        "áœµ",
        "áœ¶",
        "á ƒ",
        "á ‰",
        "á¥„",
        "á¥…",
        "áª¨",
        "áª©",
        "áªª",
        "áª«",
        "á­š",
        "á­›",
        "á­",
        "á­Ÿ",
        "á°»",
        "á°¼",
        "á±¾",
        "á±¿",
        "â€¼",
        "â€½",
        "â‡",
        "âˆ",
        "â‰",
        "â¸®",
        "â¸¼",
        "ê“¿",
        "ê˜",
        "ê˜",
        "ê›³",
        "ê›·",
        "ê¡¶",
        "ê¡·",
        "ê£",
        "ê£",
        "ê¤¯",
        "ê§ˆ",
        "ê§‰",
        "ê©",
        "ê©",
        "ê©Ÿ",
        "ê«°",
        "ê«±",
        "ê¯«",
        "ï¹’",
        "ï¹–",
        "ï¹—",
        "ï¼",
        "ï¼",
        "ï¼Ÿ",
        "ğ©–",
        "ğ©—",
        "ğ‘‡",
        "ğ‘ˆ",
        "ğ‘‚¾",
        "ğ‘‚¿",
        "ğ‘ƒ€",
        "ğ‘ƒ",
        "ğ‘…",
        "ğ‘…‚",
        "ğ‘…ƒ",
        "ğ‘‡…",
        "ğ‘‡†",
        "ğ‘‡",
        "ğ‘‡",
        "ğ‘‡Ÿ",
        "ğ‘ˆ¸",
        "ğ‘ˆ¹",
        "ğ‘ˆ»",
        "ğ‘ˆ¼",
        "ğ‘Š©",
        "ğ‘‘‹",
        "ğ‘‘Œ",
        "ğ‘—‚",
        "ğ‘—ƒ",
        "ğ‘—‰",
        "ğ‘—Š",
        "ğ‘—‹",
        "ğ‘—Œ",
        "ğ‘—",
        "ğ‘—",
        "ğ‘—",
        "ğ‘—",
        "ğ‘—‘",
        "ğ‘—’",
        "ğ‘—“",
        "ğ‘—”",
        "ğ‘—•",
        "ğ‘—–",
        "ğ‘——",
        "ğ‘™",
        "ğ‘™‚",
        "ğ‘œ¼",
        "ğ‘œ½",
        "ğ‘œ¾",
        "ğ‘©‚",
        "ğ‘©ƒ",
        "ğ‘ª›",
        "ğ‘ªœ",
        "ğ‘±",
        "ğ‘±‚",
        "ğ–©®",
        "ğ–©¯",
        "ğ–«µ",
        "ğ–¬·",
        "ğ–¬¸",
        "ğ–­„",
        "ğ›²Ÿ",
        "ğªˆ",
    ]

    extra_tokens += other_tokens

    regex = re.compile(r"|".join(extra_tokens))

    def tokenize(self, content: str, language: str = "english") -> Dict[str, List[Any]]:

        sentences_first = nltk.sent_tokenize(content, language)
        sentences = []

        for sentence in sentences_first:
            s = self.regex.split(sentence)
            sentences += s

        ret = {"sentences": sentences}

        return ret

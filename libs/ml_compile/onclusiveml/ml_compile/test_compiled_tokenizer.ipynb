{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/repositories/ml-mesh/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.neuron\n",
    "from compiled_tokenizer import CompiledTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get original tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 3293, 83, 142, 27781, 149357, 5, 1401, 4527, 442, 47, 69101, 47, 1098, 52825, 7, 136, 2363, 375, 5974, 297, 20117, 7, 5, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "sample_input = 'This is an example sentence. We use it to compare tokenizers and their compiled variants.'\n",
    "\n",
    "tokenizer_output = tokenizer(sample_input)\n",
    "tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerBase.__call__ of BertTokenizerFast(name_or_path='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./hf_tokenizer/tokenizer_config.json',\n",
       " './hf_tokenizer/special_tokens_map.json',\n",
       " './hf_tokenizer/unigram.json',\n",
       " './hf_tokenizer/added_tokens.json',\n",
       " './hf_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./hf_tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `__init__` + inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "compiled_tokenizer_1 = CompiledTokenizer(tokenizer,max_length=MAX_LENGTH)\n",
    "compiled_tokenizer_1_output = compiled_tokenizer_1(sample_input)\n",
    "compiled_tokenizer_1_torch_output = compiled_tokenizer_1(sample_input,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(compiled_tokenizer.CompiledTokenizer,\n",
       " {'padding': 'max_length',\n",
       "  'truncation': True,\n",
       "  'add_special_tokens': True,\n",
       "  'max_length': 50},\n",
       " True,\n",
       " <bound method CompiledTokenizer.__call__ of <compiled_tokenizer.CompiledTokenizer object at 0x7ff0372d6130>>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_tokenizer_1.__class__, compiled_tokenizer_1.tokenization_settings, compiled_tokenizer_1.compiled, compiled_tokenizer_1.__call__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `.from_tokenizer` + inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "compiled_tokenizer_2 = CompiledTokenizer.from_tokenizer(tokenizer,max_length=MAX_LENGTH)\n",
    "compiled_tokenizer_2_output = compiled_tokenizer_2(sample_input)\n",
    "compiled_tokenizer_2_torch_output = compiled_tokenizer_2(sample_input,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(compiled_tokenizer.CompiledTokenizer,\n",
       " {'padding': 'max_length',\n",
       "  'truncation': True,\n",
       "  'add_special_tokens': True,\n",
       "  'max_length': 50},\n",
       " True,\n",
       " <bound method CompiledTokenizer.__call__ of <compiled_tokenizer.CompiledTokenizer object at 0x7ff0372d6dc0>>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_tokenizer_2.__class__, compiled_tokenizer_2.tokenization_settings, compiled_tokenizer_2.compiled, compiled_tokenizer_2.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [0, 3293, 83, 142, 27781, 149357, 5, 1401, 4527, 442, 47, 69101, 47, 1098, 52825, 7, 136, 2363, 375, 5974, 297, 20117, 7, 5, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [0, 3293, 83, 142, 27781, 149357, 5, 1401, 4527, 442, 47, 69101, 47, 1098, 52825, 7, 136, 2363, 375, 5974, 297, 20117, 7, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
       " {'input_ids': [0, 3293, 83, 142, 27781, 149357, 5, 1401, 4527, 442, 47, 69101, 47, 1098, 52825, 7, 136, 2363, 375, 5974, 297, 20117, 7, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
       " {'input_ids': tensor([[     0,   3293,     83,    142,  27781, 149357,      5,   1401,   4527,\n",
       "             442,     47,  69101,     47,   1098,  52825,      7,    136,   2363,\n",
       "             375,   5974,    297,  20117,      7,      5,      2,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])},\n",
       " {'input_ids': tensor([[     0,   3293,     83,    142,  27781, 149357,      5,   1401,   4527,\n",
       "             442,     47,  69101,     47,   1098,  52825,      7,    136,   2363,\n",
       "             375,   5974,    297,  20117,      7,      5,      2,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output, compiled_tokenizer_1_output, compiled_tokenizer_2_output, compiled_tokenizer_1_torch_output, compiled_tokenizer_2_torch_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `save_pretrained` + `from_pretrained`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_tokenizer_1.save_pretrained('compiled_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(compiled_tokenizer.CompiledTokenizer,\n",
       " {'padding': 'max_length',\n",
       "  'truncation': True,\n",
       "  'add_special_tokens': True,\n",
       "  'max_length': 50},\n",
       " True,\n",
       " <bound method CompiledTokenizer.__call__ of <compiled_tokenizer.CompiledTokenizer object at 0x7ff037821730>>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_tokenizer_reloaded = CompiledTokenizer.from_pretrained('compiled_tokenizer')\n",
    "compiled_tokenizer_reloaded.__class__, compiled_tokenizer_reloaded.tokenization_settings, compiled_tokenizer_reloaded.compiled, compiled_tokenizer_reloaded.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_tokenizer_reloaded_output = compiled_tokenizer_reloaded(sample_input)\n",
    "compiled_tokenizer_reloaded_torch_output = compiled_tokenizer_reloaded(sample_input, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_tokenizer_1_output == compiled_tokenizer_reloaded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_type in compiled_tokenizer_1_torch_output:\n",
    "    torch.all(compiled_tokenizer_1_torch_output[token_type].eq(compiled_tokenizer_2_torch_output[token_type])) # reconcile __init__ with from_pretrained\n",
    "    torch.all(compiled_tokenizer_2_torch_output[token_type].eq(compiled_tokenizer_reloaded_torch_output[token_type])) # reconcile across export and re-import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test delegated methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = compiled_tokenizer_1.encode_plus(sample_input)\n",
    "encodings = compiled_tokenizer_1.encode(sample_input)\n",
    "sample_input_reconsctructed = compiled_tokenizer_1.decode(encodings)\n",
    "token_type_ids = compiled_tokenizer_1.create_token_type_ids_from_sequences(token_ids_0=token_ids['input_ids'])\n",
    "tokens_converted_to_string = compiled_tokenizer_1.convert_tokens_to_string(token_ids)\n",
    "cleaned_string = compiled_tokenizer_1.clean_up_tokenization(\" 're 've some word ! ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True, True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    token_ids == compiled_tokenizer_1.tokenizer.encode_plus(sample_input),\n",
    "    encodings == compiled_tokenizer_1.tokenizer.encode(sample_input),\n",
    "    sample_input_reconsctructed == compiled_tokenizer_1.tokenizer.decode(encodings),\n",
    "    token_type_ids == compiled_tokenizer_1.tokenizer.create_token_type_ids_from_sequences(token_ids_0=token_ids['input_ids']),\n",
    "    tokens_converted_to_string == compiled_tokenizer_1.tokenizer.convert_tokens_to_string(token_ids),\n",
    "    cleaned_string == compiled_tokenizer_1.tokenizer.clean_up_tokenization(\" 're 've some word ! ?\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 3293, 83, 142, 27781, 149357, 5, 1401, 4527, 442, 47, 69101, 47, 1098, 52825, 7, 136, 2363, 375, 5974, 297, 20117, 7, 5, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3293, 83, 142, 27781, 149357, 5, 1401, 4527, 442]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> This is an example sentence. We use it to compare tokenizers and their compiled variants.</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_reconsctructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_idstoken_type_idsattention_mask'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_converted_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'re've some word!?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

---

services:
  # =============================================
  # SERVE
  # =============================================

  # serving image
  serve:
    devices:
      - /dev/neuron0:/dev/neuron0
    volumes:
      - serve-volume:/projects/ner/serve/models
    build:
      context: ../../
      dockerfile: projects/ner/serve/Dockerfile
      target: ${TARGET_BUILD_STAGE:-development}
      args:
        AWS_ACCOUNT_ID: ${AWS_ACCOUNT_ID:-690763002009}
        PROJECT: ner
        COMPONENT: serve
        BASE_IMAGE_TAG: ${BASE_IMAGE_TAG:-v23.11.3}
    image: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve:${IMAGE_TAG}
    command: [python, -m, src.serve.__main__]
    env_file: serve/config/${DEPLOYMENT:-prod}.env
    profiles: [serve, functional, load]
    ports:
      - 8000:8000
    hostname: onclusiveml
    networks:
      - onclusive-net
    healthcheck:
      test: [CMD, curl, -f, http://serve:8000/ner/v1/ready]
      interval: 10s
      retries: 5
      start_period: 5s
      timeout: 10s

  # utility service: downloading the compiled model artifact that will be served
  serve-download-model:
    volumes:
      - serve-volume:/projects/ner/serve/models
    image: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve:${IMAGE_TAG}
    command: [python, -m, src.download]
    profiles: [serve, integration, functional, load]
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      ONCLUSIVEML_NEPTUNE_API_TOKEN: ${NEPTUNE_API_TOKEN}
      NEPTUNE_CLIENT_MODE: read-only
    env_file: serve/config/${DEPLOYMENT:-prod}.env
    hostname: onclusiveml
    networks: [onclusive-net]

  # unit tests
  # - do NOT include model artifact
  # - do NOT include REST model server process
  serve-unit:
    image: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve:${IMAGE_TAG}
    command: [python, -m, pytest, tests/unit, -ra, -vv, --capture=no]
    profiles: [unit]
    hostname: onclusiveml
    env_file: serve/config/${DEPLOYMENT:-prod}.env
    networks:
      - onclusive-net

  # integration tests
  # - include model artifact
  # - do NOT include REST model server process
  serve-integration:
    devices:
      - /dev/neuron0:/dev/neuron0
    volumes:
      - serve-volume:/projects/ner/serve/models
    image: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve:${IMAGE_TAG}
    command: [python, -m, pytest, tests/integration, -ra, -vv, --capture=no]
    env_file: serve/config/${DEPLOYMENT:-prod}.env
    profiles: [integration]
    hostname: onclusiveml
    networks:
      - onclusive-net

  # functional tests
  # - include model artifact
  # - include REST model server process
  serve-functional:
    volumes:
      - serve-volume:/projects/ner/serve/models
    image: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve:${IMAGE_TAG}
    command: [python, -m, pytest, tests/functional, -ra, -vv, --capture=no]
    env_file: serve/config/${DEPLOYMENT:-prod}.env
    profiles: [functional]
    hostname: onclusiveml
    networks:
      - onclusive-net
    depends_on:
      serve:
        condition: service_healthy

  # load tests
  # - include model artifact
  # - include REST model server process
  serve-load:
    volumes:
      - serve-volume:/projects/ner/serve/models
    image: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve:${IMAGE_TAG}
    command: [python, -m, pytest, tests/load, -ra, -vv, --capture=no]
    environment:
      # load test params
      ONCLUSIVEML_SERVING_NUM_USERS: 2 # numbers of users in load test
      ONCLUSIVEML_SERVING_SPAWN_RATE: 2 # per second spawn rate of clients until `num_users` is reached
      ONCLUSIVEML_SERVING_HOST: http://serve:8000 # base url of the host
      ONCLUSIVEML_SERVING_RESET_STATS: No
      ONCLUSIVEML_SERVING_RUN_TIME: 45
      ONCLUSIVEML_SERVING_LOGLEVEL: INFO
    env_file: serve/config/${DEPLOYMENT:-prod}.env
    profiles: [load]
    hostname: onclusiveml
    networks:
      - onclusive-net
    depends_on:
      serve:
        condition: service_healthy

  # utility service: uploading the relevant test results to the model registry
  serve-upload-results:
    volumes:
      - serve-volume:/projects/ner/serve/models
    image: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve:${IMAGE_TAG}
    command: [python, -m, src.upload]
    profiles: [serve]
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      ONCLUSIVEML_NEPTUNE_API_TOKEN: ${NEPTUNE_API_TOKEN}
      NEPTUNE_CLIENT_MODE: async
      DOCKER_IMAGE_NAME: ${AWS_ACCOUNT_ID:-690763002009}.dkr.ecr.us-east-1.amazonaws.com/ner-serve
      DOCKER_IMAGE_TAG: ${IMAGE_TAG}
      DOCKER_COMPOSE_ENVIRONMENT: ${ENVIRONMENT}
      GITHUB_REPOSITORY: ${GITHUB_REPOSITORY}
      GITHUB_ACTOR: ${GITHUB_ACTOR}
      GITHUB_ENV: ${GITHUB_ENV}
      GITHUB_WORKFLOW: ${GITHUB_WORKFLOW}
      GITHUB_JOB: ${GITHUB_JOB}
      GITHUB_REF: ${GITHUB_REF}
      GITHUB_BASE_REF: ${GITHUB_BASE_REF}
      GITHUB_HEAD_REF: ${GITHUB_HEAD_REF}
      GITHUB_SHA: ${GITHUB_SHA}
      GITHUB_EVENT_NAME: ${GITHUB_EVENT_NAME}
      GITHUB_RUN_ID: ${GITHUB_RUN_ID}
      GITHUB_RUN_NUMBER: ${GITHUB_RUN_NUMBER}
      RUNNER_ARCH: ${RUNNER_ARCH}
      RUNNER_NAME: ${RUNNER_NAME}
      RUNNER_OS: ${RUNNER_OS}
    env_file: serve/config/${DEPLOYMENT:-prod}.env
    hostname: onclusive-ml
    networks: [onclusive-net]

networks:
  onclusive-net:

volumes :
  serve-volume:

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/repositories/ml-mesh/.py_38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "from transformers import pipeline\n",
    "from keybert import KeyBERT\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 645/645 [00:00<00:00, 73.6kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 471M/471M [00:05<00:00, 86.1MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 480/480 [00:00<00:00, 143kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 9.08M/9.08M [00:00<00:00, 64.6MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 69.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "# --- initialize models\n",
    "# get specified huggingface transformer pipeline\n",
    "HF_MODEL_REFERENCE = os.environ.get('HF_MODEL_REFERENCE','sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "hf_pipeline = pipeline(\"feature-extraction\", model=HF_MODEL_REFERENCE)\n",
    "\n",
    "# initialize keybert model with huggingface pipeline backend\n",
    "keybert_model = KeyBERT(model=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create prediction files\n",
    "sample_inputs = [\n",
    "    '''Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "         the learning algorithm to generalize from the training data to unseen situations in a\n",
    "         'reasonable' way (see inductive bias).''',\n",
    "    '''Überwachtes Lernen ist die maschinelle Lernaufgabe, eine Funktion zu lernen, die\n",
    "         ordnet eine Eingabe einer Ausgabe basierend auf beispielhaften Eingabe-Ausgabe-Paaren zu. Es folgert a\n",
    "         Funktion aus beschrifteten Trainingsdaten, die aus einer Reihe von Trainingsbeispielen bestehen.\n",
    "         Beim überwachten Lernen ist jedes Beispiel ein Paar, das aus einem Eingabeobjekt besteht\n",
    "         (typischerweise ein Vektor) und einem gewünschten Ausgangswert (auch Überwachungssignal genannt).\n",
    "         Ein überwachter Lernalgorithmus analysiert die Trainingsdaten und erzeugt eine abgeleitete Funktion.\n",
    "         die zum Mapping neuer Beispiele verwendet werden können. Ein optimales Szenario ermöglicht die\n",
    "         Algorithmus, um die Klassenbezeichnungen für unsichtbare Instanzen korrekt zu bestimmen. Dafür braucht man\n",
    "         den Lernalgorithmus zum Verallgemeinern der Trainingsdaten auf ungesehene Situationen in a\n",
    "         'vernünftiger' Weg (siehe induktive Vorspannung).''',\n",
    "    '''El aprendizaje supervisado es la tarea de aprendizaje automático de aprender una función que\n",
    "         asigna una entrada a una salida en función de pares de entrada-salida de ejemplo. Se infiere un\n",
    "         función a partir de datos de entrenamiento etiquetados que consisten en un conjunto de ejemplos de entrenamiento.\n",
    "         En el aprendizaje supervisado, cada ejemplo es un par que consta de un objeto de entrada\n",
    "         (típicamente un vector) y un valor de salida deseado (también llamado señal de supervisión).\n",
    "         Un algoritmo de aprendizaje supervisado analiza los datos de entrenamiento y produce una función inferida,\n",
    "         que se puede utilizar para mapear nuevos ejemplos. Un escenario óptimo permitirá que la\n",
    "         algoritmo para determinar correctamente las etiquetas de clase para instancias no vistas. Esto requiere\n",
    "         el algoritmo de aprendizaje para generalizar a partir de los datos de entrenamiento a situaciones no vistas en un\n",
    "         manera 'razonable' (ver sesgo inductivo).'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0 length: 222\n",
      "Attention mask 0 length: 222\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence 1 length: 222\n",
      "Attention mask 1 length: 222\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sequence 2 length: 222\n",
      "Attention mask 2 length: 222\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "tokenized_inputs = hf_pipeline.tokenizer(sample_inputs, truncation=True, padding=True) # will truncate to max token length of batch\n",
    "\n",
    "for i in range(len(sample_inputs)):\n",
    "    print(f\"Sequence {i} length: {len(tokenized_inputs['input_ids'][i])}\")\n",
    "    print(f\"Attention mask {i} length: {len(tokenized_inputs['attention_mask'][i])}\")\n",
    "    print(f\"Attention mask: {tokenized_inputs['attention_mask'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "tokenized_inputs_tensor = hf_pipeline.tokenizer(sample_inputs, padding='max_length',return_tensors='pt')\n",
    "model_predictions = hf_pipeline.model(**tokenized_inputs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1341274082660675,\n",
       " -0.3512396216392517,\n",
       " -0.29578202962875366,\n",
       " -0.1361517608165741,\n",
       " 0.30853715538978577,\n",
       " 0.25779828429222107,\n",
       " 0.5945106148719788,\n",
       " -0.08795639127492905,\n",
       " -0.37195488810539246,\n",
       " -0.19017291069030762]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions.last_hidden_state.tolist()[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface pipeline model\n",
    "hf_pipeline_predictions = hf_pipeline(sample_inputs,  truncation=True, padding='max_length') # padding & truncation doesnt seem to affect sequence length dim of extracted features, which makes sense as no embedding applies for paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hf_pipeline_predictions), len(hf_pipeline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf_pipeline_predictions[0]), len(hf_pipeline_predictions[1]), len(hf_pipeline_predictions[2]) \n",
    "# -> (n_batch x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 <class 'list'>\n",
      "222 <class 'list'>\n",
      "198 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_pipeline_predictions[0][0]), type(hf_pipeline_predictions[0][0])) # 177\n",
    "print(len(hf_pipeline_predictions[1][0]), type(hf_pipeline_predictions[1][0])) # 222\n",
    "print(len(hf_pipeline_predictions[2][0]), type(hf_pipeline_predictions[2][0])) # 198\n",
    "# -> n_batch x 1 x n_tokens x ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 <class 'list'>\n",
      "384 <class 'list'>\n",
      "384 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(hf_pipeline_predictions[0][0][0]), type(hf_pipeline_predictions[0][0][0])) # 384\n",
    "print(len(hf_pipeline_predictions[1][0][0]), type(hf_pipeline_predictions[1][0][0])) # 384\n",
    "print(len(hf_pipeline_predictions[2][0][0]), type(hf_pipeline_predictions[2][0][0])) # 384\n",
    "# -> n_batch x 1 x n_tokens x 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1341274380683899,\n",
       " -0.3512396514415741,\n",
       " -0.2957819998264313,\n",
       " -0.13615182042121887,\n",
       " 0.30853718519210815,\n",
       " 0.25779861211776733,\n",
       " 0.594510555267334,\n",
       " -0.08795640617609024,\n",
       " -0.3719547390937805,\n",
       " -0.1901727169752121]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_pipeline_predictions[0][0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BertTokenizerFast(name_or_path='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}),\n",
       " <bound method Module.parameters of BertModel(\n",
       "   (embeddings): BertEmbeddings(\n",
       "     (word_embeddings): Embedding(250037, 384, padding_idx=0)\n",
       "     (position_embeddings): Embedding(512, 384)\n",
       "     (token_type_embeddings): Embedding(2, 384)\n",
       "     (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (encoder): BertEncoder(\n",
       "     (layer): ModuleList(\n",
       "       (0-11): 12 x BertLayer(\n",
       "         (attention): BertAttention(\n",
       "           (self): BertSelfAttention(\n",
       "             (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (output): BertSelfOutput(\n",
       "             (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "             (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (intermediate): BertIntermediate(\n",
       "           (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (intermediate_act_fn): GELUActivation()\n",
       "         )\n",
       "         (output): BertOutput(\n",
       "           (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (pooler): BertPooler(\n",
       "     (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "     (activation): Tanh()\n",
       "   )\n",
       " )>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_pipeline.tokenizer, hf_pipeline.model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hf_pipeline_predictions[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keybert model\n",
    "keybert_predictions = keybert_model.extract_keywords(sample_inputs, keyphrase_ngram_range=(1, 1), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('supervised', 0.4765),\n",
       "  ('learning', 0.4647),\n",
       "  ('instances', 0.4511),\n",
       "  ('training', 0.4236),\n",
       "  ('supervisory', 0.393)],\n",
       " [('trainingsbeispielen', 0.534),\n",
       "  ('algorithmus', 0.5079),\n",
       "  ('trainingsdaten', 0.4838),\n",
       "  ('lernalgorithmus', 0.4746),\n",
       "  ('lernen', 0.4611)],\n",
       " [('aprendizaje', 0.4988),\n",
       "  ('algoritmo', 0.4943),\n",
       "  ('entrenamiento', 0.4444),\n",
       "  ('supervisado', 0.4184),\n",
       "  ('aprender', 0.4124)]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keybert_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9befd1b81d15041444cc6b9f39ee66ccde79efac809d38dfd545818bbc49524f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

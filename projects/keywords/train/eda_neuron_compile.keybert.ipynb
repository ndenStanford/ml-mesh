{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.pipelines import pipeline\n",
    "from time import time\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- initialize models\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "hf_pipeline = pipeline(\n",
    "    \"feature-extraction\", \n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create prediction files\n",
    "sample_inputs = [\n",
    "    '''Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "         the learning algorithm to generalize from the training data to unseen situations in a\n",
    "         'reasonable' way (see inductive bias).''',\n",
    "    '''Überwachtes Lernen ist die maschinelle Lernaufgabe, eine Funktion zu lernen, die\n",
    "         ordnet eine Eingabe einer Ausgabe basierend auf beispielhaften Eingabe-Ausgabe-Paaren zu. Es folgert a\n",
    "         Funktion aus beschrifteten Trainingsdaten, die aus einer Reihe von Trainingsbeispielen bestehen.\n",
    "         Beim überwachten Lernen ist jedes Beispiel ein Paar, das aus einem Eingabeobjekt besteht\n",
    "         (typischerweise ein Vektor) und einem gewünschten Ausgangswert (auch Überwachungssignal genannt).\n",
    "         Ein überwachter Lernalgorithmus analysiert die Trainingsdaten und erzeugt eine abgeleitete Funktion.\n",
    "         die zum Mapping neuer Beispiele verwendet werden können. Ein optimales Szenario ermöglicht die\n",
    "         Algorithmus, um die Klassenbezeichnungen für unsichtbare Instanzen korrekt zu bestimmen. Dafür braucht man\n",
    "         den Lernalgorithmus zum Verallgemeinern der Trainingsdaten auf ungesehene Situationen in a\n",
    "         'vernünftiger' Weg (siehe induktive Vorspannung).''',\n",
    "    '''El aprendizaje supervisado es la tarea de aprendizaje automático de aprender una función que\n",
    "         asigna una entrada a una salida en función de pares de entrada-salida de ejemplo. Se infiere un\n",
    "         función a partir de datos de entrenamiento etiquetados que consisten en un conjunto de ejemplos de entrenamiento.\n",
    "         En el aprendizaje supervisado, cada ejemplo es un par que consta de un objeto de entrada\n",
    "         (típicamente un vector) y un valor de salida deseado (también llamado señal de supervisión).\n",
    "         Un algoritmo de aprendizaje supervisado analiza los datos de entrenamiento y produce una función inferida,\n",
    "         que se puede utilizar para mapear nuevos ejemplos. Un escenario óptimo permitirá que la\n",
    "         algoritmo para determinar correctamente las etiquetas de clase para instancias no vistas. Esto requiere\n",
    "         el algoritmo de aprendizaje para generalizar a partir de los datos de entrenamiento a situaciones no vistas en un\n",
    "         manera 'razonable' (ver sesgo inductivo).'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron trace the model\n",
    "tokenizer_settings = {\n",
    "    'max_length':10,\n",
    "    'padding':'max_length',\n",
    "    'truncation':True,\n",
    "    'add_special_tokens':False\n",
    "}\n",
    "\n",
    "tokenized_inputs = tokenizer(sample_inputs[0], return_tensors='pt',**tokenizer_settings)\n",
    "tokenized_inputs_for_tracing = tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], tokenized_inputs['token_type_ids']\n",
    "model_predictions = model(**tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:There are 3 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)\n",
      "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 563, fused = 546, percent fused = 96.98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/repositories/ml-serve-app/pytorch_venv/lib64/python3.7/site-packages/torch_neuron/ops/aten.py:1796: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Compiling function _NeuronGraph$688 with neuron-cc\n",
      "INFO:Neuron:Compiling with command line: '/home/ec2-user/repositories/ml-serve-app/pytorch_venv/bin/neuron-cc compile /tmp/tmp2123eycx/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmp2123eycx/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 10, 384], \"float32\"], \"1:0\": [[1, 1, 1, 10], \"float32\"]}, \"outputs\": [\"batchnorm_24/add_1:0\", \"Tanh_12:0\"]} --verbose 35'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "....\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 563, compiled = 546, percent compiled = 96.98%\n",
      "INFO:Neuron:The neuron partitioner created 1 sub-graphs\n",
      "INFO:Neuron:Neuron successfully compiled 1 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 100.0%\n",
      "INFO:Neuron:Compiled these operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 96\n",
      "INFO:Neuron: => aten::add: 36\n",
      "INFO:Neuron: => aten::contiguous: 12\n",
      "INFO:Neuron: => aten::div: 12\n",
      "INFO:Neuron: => aten::dropout: 37\n",
      "INFO:Neuron: => aten::gelu: 12\n",
      "INFO:Neuron: => aten::layer_norm: 25\n",
      "INFO:Neuron: => aten::linear: 73\n",
      "INFO:Neuron: => aten::matmul: 24\n",
      "INFO:Neuron: => aten::permute: 48\n",
      "INFO:Neuron: => aten::select: 1\n",
      "INFO:Neuron: => aten::size: 96\n",
      "INFO:Neuron: => aten::slice: 1\n",
      "INFO:Neuron: => aten::softmax: 12\n",
      "INFO:Neuron: => aten::tanh: 1\n",
      "INFO:Neuron: => aten::transpose: 12\n",
      "INFO:Neuron: => aten::view: 48\n",
      "INFO:Neuron:Not compiled operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 1 [supported]\n",
      "INFO:Neuron: => aten::add: 3 [supported]\n",
      "INFO:Neuron: => aten::embedding: 3 [not supported]\n",
      "INFO:Neuron: => aten::mul: 1 [supported]\n",
      "INFO:Neuron: => aten::rsub: 1 [supported]\n",
      "INFO:Neuron: => aten::size: 1 [supported]\n",
      "INFO:Neuron: => aten::slice: 4 [supported]\n",
      "INFO:Neuron: => aten::to: 1 [supported]\n",
      "INFO:Neuron: => aten::unsqueeze: 2 [supported]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.jit._trace.TopLevelTracedModule"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_model = torch.neuron.trace(model, tokenized_inputs_for_tracing, strict=False)\n",
    "\n",
    "neuron_model.save(f\"./{tokenizer_settings['max_length']}_neuron_traced_paraphrase-multilingual-MiniLM-L12-v2.pt\")\n",
    "type(neuron_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.jit._script.RecursiveScriptModule"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_model = torch.jit.load(f\"./{tokenizer_settings['max_length']}_neuron_traced_paraphrase-multilingual-MiniLM-L12-v2.pt\")\n",
    "type(neuron_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_predictions_2 = neuron_model(*tokenized_inputs_for_tracing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_predictions = hf_pipeline(sample_inputs, tokenize_kwargs=tokenizer_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipeline_predictions[0][0]) # n_batch x 1 x n_tokens x n_embed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF pipeline and HF keybert latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency hf pipeline with fixed tokenization length: 73.57719898223877ms\n"
     ]
    }
   ],
   "source": [
    "n_test = 50\n",
    "\n",
    "hf_start = time()\n",
    "for i in range(n_test):\n",
    "    hf_pipeline(sample_inputs, tokenize_kwargs = tokenizer_settings)\n",
    "hf_end = time()\n",
    "\n",
    "print(f'Average latency hf pipeline with fixed tokenization length: {(hf_end - hf_start)/n_test * 1000}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency hf pipeline with dynamic tokenization length: 69.37009334564209ms\n"
     ]
    }
   ],
   "source": [
    "n_test = 50\n",
    "\n",
    "hf_start = time()\n",
    "for i in range(n_test):\n",
    "    hf_pipeline(sample_inputs)\n",
    "hf_end = time()\n",
    "\n",
    "print(f'Average latency hf pipeline with dynamic tokenization length: {(hf_end - hf_start)/n_test * 1000}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency hf keybert with fixed tokenization length: 1786.524739265442ms\n"
     ]
    }
   ],
   "source": [
    "hf_keybert = KeyBERT(model=hf_pipeline)\n",
    "\n",
    "n_test = 50\n",
    "\n",
    "hf_start = time()\n",
    "for i in range(n_test):\n",
    "    hf_keywords = hf_keybert.extract_keywords(sample_inputs, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "hf_end = time()\n",
    "\n",
    "print(f'Average latency hf keybert with fixed tokenization length: {(hf_end - hf_start)/n_test * 1000}ms')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron pipeline and neuron keybert latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ec2-user/repositories/ml-mesh/')\n",
    "from libs.keyword.onclusiveml.keyword.compile_pipeline import compile_pipeline\n",
    "\n",
    "neuron_pipeline = compile_pipeline(\n",
    "    pipeline=hf_pipeline,\n",
    "    traced_model=neuron_model,\n",
    "    tokenizer_settings=tokenizer_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency neuron pipeline with fixed tokenization length: 8.94350528717041ms\n"
     ]
    }
   ],
   "source": [
    "n_test = 50\n",
    "\n",
    "neuron_start = time()\n",
    "for i in range(n_test):\n",
    "    neuron_pipeline(sample_inputs)\n",
    "neuron_end = time()\n",
    "\n",
    "print(f'Average latency neuron pipeline with fixed tokenization length: {(neuron_end - neuron_start)/n_test * 1000}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency neuron keybert with fixed tokenization length: 670.1397895812988ms\n"
     ]
    }
   ],
   "source": [
    "neuron_keybert = KeyBERT(model=neuron_pipeline)\n",
    "\n",
    "n_test = 50\n",
    "\n",
    "neuron_start = time()\n",
    "for i in range(n_test):\n",
    "    neuron_keywords = neuron_keybert.extract_keywords(sample_inputs, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "neuron_end = time()\n",
    "\n",
    "print(f'Average latency neuron keybert with fixed tokenization length: {(neuron_end - neuron_start)/n_test * 1000}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.jit._script.RecursiveScriptModule"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(neuron_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Neuron PyTorch)",
   "language": "python",
   "name": "pytorch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa90fdc69a7259dacce8beb6bc291d33feb809e0f8f9c6ebe0c1ec1fa0cc52bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from keybert import KeyBERT\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import torch\n",
    "\n",
    "local_output_dir = os.environ.get('LOCAL_OUTPUT_DIR',os.path.join('.','keyword_model_artifacts'))\n",
    "if not os.path.isdir(local_output_dir):\n",
    "    os.makedirs(local_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- initialize models\n",
    "# get specified huggingface transformer pipeline\n",
    "HF_MODEL_REFERENCE = os.environ.get('HF_MODEL_REFERENCE','sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "hf_pipeline = pipeline(\"feature-extraction\", HF_MODEL_REFERENCE)\n",
    "hf_tokenizer = hf_pipeline.tokenizer\n",
    "hf_model = hf_pipeline.model\n",
    "\n",
    "# initialize keybert model with huggingface pipeline backend\n",
    "keybert_model = KeyBERT(model=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create prediction files\n",
    "sample_inputs = [\n",
    "    '''Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "         the learning algorithm to generalize from the training data to unseen situations in a\n",
    "         'reasonable' way (see inductive bias).''',\n",
    "    '''Überwachtes Lernen ist die maschinelle Lernaufgabe, eine Funktion zu lernen, die\n",
    "         ordnet eine Eingabe einer Ausgabe basierend auf beispielhaften Eingabe-Ausgabe-Paaren zu. Es folgert a\n",
    "         Funktion aus beschrifteten Trainingsdaten, die aus einer Reihe von Trainingsbeispielen bestehen.\n",
    "         Beim überwachten Lernen ist jedes Beispiel ein Paar, das aus einem Eingabeobjekt besteht\n",
    "         (typischerweise ein Vektor) und einem gewünschten Ausgangswert (auch Überwachungssignal genannt).\n",
    "         Ein überwachter Lernalgorithmus analysiert die Trainingsdaten und erzeugt eine abgeleitete Funktion.\n",
    "         die zum Mapping neuer Beispiele verwendet werden können. Ein optimales Szenario ermöglicht die\n",
    "         Algorithmus, um die Klassenbezeichnungen für unsichtbare Instanzen korrekt zu bestimmen. Dafür braucht man\n",
    "         den Lernalgorithmus zum Verallgemeinern der Trainingsdaten auf ungesehene Situationen in a\n",
    "         'vernünftiger' Weg (siehe induktive Vorspannung).''',\n",
    "    '''El aprendizaje supervisado es la tarea de aprendizaje automático de aprender una función que\n",
    "         asigna una entrada a una salida en función de pares de entrada-salida de ejemplo. Se infiere un\n",
    "         función a partir de datos de entrenamiento etiquetados que consisten en un conjunto de ejemplos de entrenamiento.\n",
    "         En el aprendizaje supervisado, cada ejemplo es un par que consta de un objeto de entrada\n",
    "         (típicamente un vector) y un valor de salida deseado (también llamado señal de supervisión).\n",
    "         Un algoritmo de aprendizaje supervisado analiza los datos de entrenamiento y produce una función inferida,\n",
    "         que se puede utilizar para mapear nuevos ejemplos. Un escenario óptimo permitirá que la\n",
    "         algoritmo para determinar correctamente las etiquetas de clase para instancias no vistas. Esto requiere\n",
    "         el algoritmo de aprendizaje para generalizar a partir de los datos de entrenamiento a situaciones no vistas en un\n",
    "         manera 'razonable' (ver sesgo inductivo).'''\n",
    "]\n",
    "\n",
    "tokenizer_settings = {\n",
    "     'truncation': True,\n",
    "     'padding':'max_length',\n",
    "     \"add_special_tokens\": True,\n",
    "     \"max_length\": 512,\n",
    "}\n",
    "\n",
    "# tokenizer\n",
    "# has max_token_length=512\n",
    "# classic tokenizer output dict with keys 'input_ids'->List[Tuple[int]] and 'attention_mask'->:List[Tuple[int]],\n",
    "# if not setting padding = max_length, this would give a list with each array being of dim \n",
    "# n_batch x min(max(n_tokens),max_token_length) = n_batch x min(max(n_tokens),512),\n",
    "# where max(n_tokens) is the maximum sequence length across the batch\n",
    "# setting padding = max_length ensures all arrays have the same dimension\n",
    "tokenized_sample_inputs: Dict[str,List[Tuple[int]]] = dict(hf_pipeline.tokenizer(sample_inputs, **tokenizer_settings))\n",
    "tokenized_sample_inputs_tensor: Dict[str,torch.Tensor] = hf_pipeline.tokenizer(sample_inputs, return_tensors='pt', **tokenizer_settings)\n",
    "\n",
    "# hugginface transformer model\n",
    "# has n_embed=384\n",
    "# torch tensor converted to nested list of dim n_batch x max_token_length x n_embed = n_batch x 512 x 384\n",
    "hf_model_predictions: List[Tuple[Tuple[float]]] = hf_pipeline.model(**tokenized_sample_inputs_tensor).last_hidden_state.tolist()\n",
    "\n",
    "# huggingface pipeline\n",
    "# note: this returns an array as nested lists of dim n_batch x 1 x n_tokens x n_embed = n_batch x 1 x min(n_tokens,512) x 384\n",
    "# the n_tokens dim is input dependent regardless of padding approach chosen for tokenization, as the\n",
    "# embeddings of the trivial padding tokens seem to get removed by the head section of the pipeline wrapper\n",
    "hf_pipeline_predictions: List[List[Tuple[float]]] = hf_pipeline(sample_inputs, tokenize_kwargs=tokenizer_settings) \n",
    "\n",
    "# keybert model\n",
    "# list of tuples (ngram/word, prob)\n",
    "keybert_predictions: List[Tuple[str,float]] = keybert_model.extract_keywords(sample_inputs, keyphrase_ngram_range=(1, 1), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://new-ui.neptune.ai/onclusive/keyword-extraction/m/KEY-KEYBERT/v/KEY-KEYBERT-18\n",
      "Uploading text_inputs from local path ./keyword_model_artifacts/text_inputs.json to meta data path model/test_files/text_inputs.\n",
      "Uploaded text_inputs from local path ./keyword_model_artifacts/text_inputs.json to meta data path model/test_files/text_inputs\n",
      "Uploading tokenizer_settings from local path ./keyword_model_artifacts/tokenizer_settings.json to meta data path model/test_files/tokenizer_settings.\n",
      "Uploaded tokenizer_settings from local path ./keyword_model_artifacts/tokenizer_settings.json to meta data path model/test_files/tokenizer_settings\n",
      "Uploading tokenized_inputs from local path ./keyword_model_artifacts/tokenized_inputs.json to meta data path model/test_files/tokenized_inputs.\n",
      "Uploaded tokenized_inputs from local path ./keyword_model_artifacts/tokenized_inputs.json to meta data path model/test_files/tokenized_inputs\n",
      "Uploading hf_model_predictions from local path ./keyword_model_artifacts/hf_model_predictions.json to meta data path model/test_files/hf_model_predictions.\n",
      "Uploaded hf_model_predictions from local path ./keyword_model_artifacts/hf_model_predictions.json to meta data path model/test_files/hf_model_predictions\n",
      "Uploading hf_pipeline_predictions from local path ./keyword_model_artifacts/hf_pipeline_predictions.json to meta data path model/test_files/hf_pipeline_predictions.\n",
      "Uploaded hf_pipeline_predictions from local path ./keyword_model_artifacts/hf_pipeline_predictions.json to meta data path model/test_files/hf_pipeline_predictions\n",
      "Uploading keybert_predictions from local path ./keyword_model_artifacts/keybert_predictions.json to meta data path model/test_files/keybert_predictions.\n",
      "Uploaded keybert_predictions from local path ./keyword_model_artifacts/keybert_predictions.json to meta data path model/test_files/keybert_predictions\n"
     ]
    }
   ],
   "source": [
    "# --- register model on neptune ai\n",
    "model_version = neptune.init_model_version(\n",
    "    model=\"KEY-KEYBERT\",\n",
    "    project=\"onclusive/keyword-extraction\",\n",
    "    api_token=\"github-ci@onclusive-api-token-value-here\", # your credentials\n",
    ")\n",
    "\n",
    "# inputs & outputs\n",
    "for (data, data_file_reference) in [\n",
    "    (sample_inputs,'text_inputs'),\n",
    "    (tokenizer_settings, 'tokenizer_settings'),\n",
    "    (tokenized_sample_inputs,'tokenized_inputs'),\n",
    "    (hf_model_predictions, 'hf_model_predictions'),\n",
    "    (hf_pipeline_predictions, 'hf_pipeline_predictions'),\n",
    "    (keybert_predictions, 'keybert_predictions'),\n",
    "    ]:\n",
    "    \n",
    "    # export locally to make use of neptune ai's uoload method\n",
    "    test_file_path = os.path.join(local_output_dir,f'{data_file_reference}.json')\n",
    "    \n",
    "    with open(test_file_path,'w') as local_file:\n",
    "        json.dump(data,local_file)\n",
    "        \n",
    "    neptune_data_reference = f'model/test_files/{data_file_reference}'\n",
    "    \n",
    "    print(f'Uploading {data_file_reference} from local path {test_file_path} to meta data path {neptune_data_reference}.')\n",
    "    \n",
    "    model_version[neptune_data_reference].upload(test_file_path)\n",
    "        \n",
    "    print(f'Uploaded {data_file_reference} from local path {test_file_path} to meta data path {neptune_data_reference}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./keyword_model_artifacts/hf_pipeline/tokenizer.json to meta data path model/hf_pipeline/tokenizer.json.\n",
      "Uploaded ./keyword_model_artifacts/hf_pipeline/tokenizer.json to meta data path model/hf_pipeline/tokenizer.json.\n",
      "Uploading ./keyword_model_artifacts/hf_pipeline/tokenizer_config.json to meta data path model/hf_pipeline/tokenizer_config.json.\n",
      "Uploaded ./keyword_model_artifacts/hf_pipeline/tokenizer_config.json to meta data path model/hf_pipeline/tokenizer_config.json.\n",
      "Uploading ./keyword_model_artifacts/hf_pipeline/special_tokens_map.json to meta data path model/hf_pipeline/special_tokens_map.json.\n",
      "Uploaded ./keyword_model_artifacts/hf_pipeline/special_tokens_map.json to meta data path model/hf_pipeline/special_tokens_map.json.\n",
      "Uploading ./keyword_model_artifacts/hf_pipeline/unigram.json to meta data path model/hf_pipeline/unigram.json.\n",
      "Uploaded ./keyword_model_artifacts/hf_pipeline/unigram.json to meta data path model/hf_pipeline/unigram.json.\n",
      "Uploading ./keyword_model_artifacts/hf_pipeline/pytorch_model.bin to meta data path model/hf_pipeline/pytorch_model.bin.\n",
      "Uploaded ./keyword_model_artifacts/hf_pipeline/pytorch_model.bin to meta data path model/hf_pipeline/pytorch_model.bin.\n",
      "Uploading ./keyword_model_artifacts/hf_pipeline/config.json to meta data path model/hf_pipeline/config.json.\n",
      "Uploaded ./keyword_model_artifacts/hf_pipeline/config.json to meta data path model/hf_pipeline/config.json.\n"
     ]
    }
   ],
   "source": [
    "# upload huggingface artifacts\n",
    "for (artifact, artifact_reference) in (\n",
    "    (hf_pipeline,'hf_pipeline'),\n",
    "):\n",
    "    artifact_local_dir = os.path.join(local_output_dir,artifact_reference)\n",
    "    artifact.save_pretrained(artifact_local_dir)\n",
    "\n",
    "    for artifact_file in os.listdir(artifact_local_dir):\n",
    "        artifact_file_path = os.path.join(artifact_local_dir,artifact_file)\n",
    "        \n",
    "        print(f'Uploading {artifact_file_path} to meta data path model/{artifact_reference}/{artifact_file}.')\n",
    "        \n",
    "        model_version[f'model/{artifact_reference}/{artifact_file}'].upload(artifact_file_path)\n",
    "        \n",
    "        print(f'Uploaded {artifact_file_path} to meta data path model/{artifact_reference}/{artifact_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 6 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 6 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/onclusive/keyword-extraction/m/KEY-KEYBERT/v/KEY-KEYBERT-18/metadata\n"
     ]
    }
   ],
   "source": [
    "model_version.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9befd1b81d15041444cc6b9f39ee66ccde79efac809d38dfd545818bbc49524f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
